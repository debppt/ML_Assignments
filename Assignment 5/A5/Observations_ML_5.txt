Observations on Overfitting Issues:
-----------------------------------
1. Overfitting in this project may occur if the model learns noise or specific patterns unique to the training images, failing to generalize to unseen test images. Signs of overfitting include:
   - High training accuracy but significantly lower testing accuracy.
   - Training loss decreasing while validation loss stagnates or increases over epochs.
2. The current CNN model uses a dropout layer with a rate of 0.28 after the fully connected layer to mitigate overfitting. This dropout layer ensures that the dense layer does not become too dependent on specific feature representations.
3. Since the dataset consists of 28×28 grayscale images of hand gestures, the model needs to learn invariant features. Without proper regularization, it could overfit due to the small size and simplicity of the dataset.

How the CNN Handles Overfitting:
--------------------------------
1. **Dropout Layer**: 
   - In the architecture, the dropout layer (rate = 0.28) after the dense layer prevents overfitting by randomly deactivating neurons during training. This reduces the risk of the network relying too much on specific paths or neurons.
2. **Max Pooling**:
   - Max pooling layers after the convolutional layers reduce the spatial dimensions, eliminating less significant details and retaining dominant features. This helps reduce the model's sensitivity to positional variance in the hand gesture images.

Impact of Adding Another Dropout Layer:
---------------------------------------
1. Adding another dropout layer (rate = 0.4) after the convolutional layers (e.g., after the second convolutional layer) can further reduce overfitting by regularizing feature extraction.
2. This forces the convolutional layers to focus on extracting meaningful patterns rather than memorizing details specific to the training data. It is expected to improve generalization for test data.

Shared Structure and Invariance Properties in This Project:
------------------------------------------------------------
1. **Shared Structure**:
   - The convolutional layers in this model employ kernels (filters) that slide across the input image, sharing weights across all spatial positions. This ensures that the network learns spatially consistent features like edges, textures, and patterns, crucial for identifying hand gestures irrespective of their position in the image.
   - For example, in this project, the 3×3 filters in the convolutional layers detect critical gesture-specific patterns, such as finger outlines or hand contours, effectively capturing shared structures in the dataset.

2. **Invariance Property**:
   - Max pooling layers introduce translational invariance by down-sampling the feature maps. This means that small shifts or distortions in the input images (e.g., slightly shifted gestures) will not significantly affect the model's output.
   - For example, in this assignment, max pooling with a 2×2 pooling size ensures that the network focuses on the overall structure of hand gestures rather than exact pixel positions, improving the robustness of classification.
